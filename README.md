# Generative-AI-Chatbot


This project involves designing and implementing a generative-based chatbot using state-of-the-art NLP architectures like BERT, DistilBERT, and BART. The goal is to build a chatbot capable of multi-turn conversations, understanding context, and providing accurate answers to user queries. This repository contains all the code, data, and instructions required to set up and use the chatbot.

## Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Dataset](#dataset)
- [Model Selection](#model-selection)
- [Preprocessing](#preprocessing)
- [User Interface](#user-interface)
- [How to Use](#how-to-use)
- [Installation](#installation)
- [Training the Model](#training-the-model)
- [Evaluation Metrics](#evaluation-metrics)
- [Future Work](#future-work)
- [Contributors](#contributors)
- [License](#license)

## Overview
The Generative AI Chatbot is a question-answering system that uses the Stanford Question Answering Dataset (SQuAD) for training. The chatbot is designed to provide accurate answers based on given context and user queries. Users can select different models and compare their responses directly through the user interface.

## Features
- **Multiple Model Support**: Choose from BERT, DistilBERT, and BART for question-answering tasks.
- **Interactive UI**: A user-friendly interface to input context and ask questions.
- **Context-Aware Responses**: Models are trained to understand the context and provide relevant answers.
- **Customizable Inputs**: Users can input any context and ask questions based on it.
- **Real-Time Comparison**: Evaluate the performance of different models with the same input.

## Dataset
The project uses the Stanford Question Answering Dataset (SQuAD), which contains over 100,000 question-answer pairs from various Wikipedia articles. The dataset is split into training, validation, and testing sets (80%, 10%, 10%).

## Model Selection
The chatbot uses three different Transformer models from the Hugging Face library:
- **BERT**: Known for its deep understanding of context and bidirectional processing.
- **DistilBERT**: A lighter version of BERT, offering faster performance with slightly reduced accuracy.
- **BART**: A sequence-to-sequence model that excels in generating text based on a given input, suitable for creating diverse and fluent responses.

## Preprocessing
Before training the models, the data undergoes several preprocessing steps to ensure compatibility, including tokenization, handling missing values, normalization, padding, and truncation. These steps ensure the input data is ready for efficient training.

## User Interface
The chatbot comes with an intuitive user interface where users can:
- Select a model (BERT, DistilBERT, BART).
- Input a context (e.g., a paragraph from a Wikipedia article).
- Ask a question related to the given context.
- Receive an answer generated by the selected model.
This interface makes it easy to experiment with different models and compare their answers to the same questions.

## How to Use
1. **Clone the Repository**:
    ```bash
    git clone https://github.com/AyushKumarSD/Generative-AI-Chatbot.git
    cd Generative-AI-Chatbot
    ```
2. **Install Dependencies**:
    See the [Installation](#installation) section for instructions.
3. **Train the Models**:
    Follow the steps in the [Training the Model](#training-the-model) section.
4. **Launch the User Interface**:
    Start the UI using Streamlit:
    ```bash
    streamlit run app.py
    ```
5. **Interact with the Chatbot**:
    Choose a model, set the context, and ask questions to see the responses.

## Installation
1. **Python**: Make sure you have Python 3.7+ installed.
2. **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```
3. **Download Pre-trained Models**: The models will be automatically downloaded when you first run the code using the Hugging Face Transformers library.

## Training the Model
To train the models:
1. Update `config.py` with the desired training parameters.
2. Run the training script:
    ```bash
    python train.py --model_type [bert|distilbert|bart]
    ```
3. Training logs and checkpoints will be saved in the `outputs` directory.

## Evaluation Metrics
The models are evaluated using:
- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Measures the overlap between generated and reference text.
- **BLEU (Bilingual Evaluation Understudy)**: Measures the precision of generated responses compared to the reference.
- **F1 Score**: Evaluates the accuracy of predicted answers.

## Future Work
- **Enhanced Model Training**: Experimenting with larger models and fine-tuning on additional datasets.
- **Evaluation & Metrics**: Implementing more advanced evaluation methods like METEOR and improving error analysis.
- **User Interface Improvements**: Adding a more interactive chat experience and real-time feedback on model performance.
- **Deployment & Monitoring**: Deploying the chatbot on a scalable cloud platform and setting up monitoring for user interactions.

## Contributors
- **Ayush Kumar** - [GitHub](https://github.com/AyushKumarSD)

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

Feel free to adjust the content based on any additional details specific to your project!
