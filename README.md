# Generative AI Chatbot

This project involves designing and implementing a generative-based chatbot using state-of-the-art NLP architectures like BERT, DistilBERT, and BART. The goal is to build a chatbot capable of multi-turn conversations, understanding context, and providing accurate answers to user queries. This repository contains all the code, data, and instructions required to set up and use the chatbot.

## Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Dataset](#dataset)
- [Model Selection](#model-selection)
- [Preprocessing](#preprocessing)
- [User Interface](#user-interface)
- [Evaluation Metrics](#evaluation-metrics)
- [Future Work](#future-work)

## Overview
The Generative AI Chatbot is a question-answering system that uses the Stanford Question Answering Dataset (SQuAD) for training. The chatbot is designed to provide accurate answers based on given context and user queries. Users can select different models and compare their responses directly through the user interface.

## Features
- **Multiple Model Support**: Choose from BERT, DistilBERT, and BART for question-answering tasks.
- **Interactive UI**: A user-friendly interface to input context and ask questions.
- **Context-Aware Responses**: Models are trained to understand the context and provide relevant answers.
- **Customizable Inputs**: Users can input any context and ask questions based on it.
- **Real-Time Comparison**: Evaluate the performance of different models with the same input.

## Dataset
The project uses the Stanford Question Answering Dataset (SQuAD), which contains over 100,000 question-answer pairs from various Wikipedia articles. The dataset is split into training, validation, and testing sets (80%, 10%, 10%).

## Model Selection
The chatbot uses three different Transformer models from the Hugging Face library:
- **BERT**: Known for its deep understanding of context and bidirectional processing.
- **DistilBERT**: A lighter version of BERT, offering faster performance with slightly reduced accuracy.
- **BART**: A sequence-to-sequence model that excels in generating text based on a given input, suitable for creating diverse and fluent responses.

## Preprocessing
Before training the models, the data undergoes several preprocessing steps to ensure compatibility, including tokenization, handling missing values, normalization, padding, and truncation. These steps ensure the input data is ready for efficient training.

## User Interface
The chatbot comes with an intuitive user interface where users can:
- Select a model (BERT, DistilBERT, BART).
- Input a context (e.g., a paragraph from a Wikipedia article).
- Ask a question related to the given context.
- Receive an answer generated by the selected model.
This interface makes it easy to experiment with different models and compare their answers to the same questions.

## Evaluation Metrics
The models are evaluated using:
- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Measures the overlap between generated and reference text.
- **BLEU (Bilingual Evaluation Understudy)**: Measures the precision of generated responses compared to the reference.
- **F1 Score**: Evaluates the accuracy of predicted answers.

## Future Work
- **Enhanced Model Training**: Experimenting with larger models and fine-tuning on additional datasets.
- **Evaluation & Metrics**: Implementing more advanced evaluation methods like METEOR and improving error analysis.
- **User Interface Improvements**: Adding a more interactive chat experience and real-time feedback on model performance.
- **Deployment & Monitoring**: Deploying the chatbot on a scalable cloud platform and setting up monitoring for user interactions.
